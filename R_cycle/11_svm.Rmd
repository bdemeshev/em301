# Метод опорных векторов в R (Support Vector Machine)

Загружаем нужные пакеты:
```{r, message=FALSE}
library(knitr)
opts_chunk$set(cache=TRUE)

library(ggplot2) # графики
library(kernlab) # Support Vector Machines
# install.packages("kernlab") # может быть нужно установить пакеты...
```

Другой популярный пакет для SVM в R --- это `e1071`.

Ядерная функция, или просто ядро, --- это скалярное произведение в преобразованном пространстве. 

\[
K(x,x')=(\phi(x),\phi(x'))
\]


```{r}
x <- 1:6
y <- 1:6
k1 <- vanilladot()
k2 <- rbfdot(sigma=1)
k1(x,y)
k2(x,y)
```

Скалярное произведение трудно интерпретировать "в лоб". Лучшая известная мне интерпретация: скалярное произведение --- это произведение длин векторов на косинус угла между ними, $(y,y')=|y|\cdot  |y'| \cos(y,y')$. 

Скалярное произведение содержит в себе всю информацию о геометрии пространства. Т.е. если знать чему равно скалярное произведение между любыми векторами, можно посчитать длину любого вектора и угол между любыми векторами. 

SVM пытается разделить гиперплоскостью наши данные в более многомерном пространстве, чем исходное, [видео](http://www.youtube.com/watch?&v=3liCbRZPrZA). Это новое пространство называется спрямляющим.

Для гауссовского ядра это спрямляющее пространство будет бесконечномерным.


Загружаем данные по стоимости квартир в Москве:
```{r}
filename <- "~/science/econometrix/em301/datasets/flats_moscow.txt"
h <- read.table(filename,header=TRUE)
str(h)
```


Для ksvm нужно указать, что зависимая переменная --- факторная, а не количественная
```{r}
h$brick <- as.factor(h$brick)
h$walk <- as.factor(h$walk)

```


Разделим выборку на две части --- 75% для обучения и 25% для оценки качества обучения. 
```{r}
n.obs <- nrow(h)
train.index <- sample(1:n.obs, round(0.75*n.obs) )
train.h <- h[train.index,]
test.h <- h[-train.index,]
```

Находим оптимальную разделяющую гиперплоскость
```{r}
m1 <- ksvm(brick~.,data=train.h,kernel="rbfdot",kpar=list(sigma=0.05),C=5)
```

Точка `.` в формуле `brick~.` означает, что мы пытаемся предсказывать переменную `brick` с помощью всех остальных. Можно было указывать привычные формулы в духе `brick~price+dist`.

Настраивать можно:
* Тип ядра, `kernel="rbfdot"`
* Параметры ядра, если они у него есть, `kpar=list(sigma=0.05)`
* Величину штрафа за неправильно классифицированные объекты, `C=5`


Вектора, лежащие на границе разделяющей полосы, называются опорными.

Найденная оптимальная гиперплоскость не даёт нам простой интепретации зависимости. Из объекта `m1` можно  извлечь опорные вектора, но в реальной практической задаче их как минимум сотни, и что с ними делать в такой ситуации мне не известно.

Строим прогнозы для тестовой части выборки:
```{r}
test.h$brick.pred <- predict(m1,test.h)
table(test.h$brick.pred,test.h$brick)
```

Есть встроенные методы подбора параметра $\sigma$ для Гауссовского ядра:
```{r}
m1 <- ksvm(brick~.,data=train.h,kernel="rbfdot",C=5)
m1
```

Слабо добраться до $\sigma$?
```{r}
kernelf(m1) # описание ядра: тип и параметры
kpar(kernelf(m1))$sigma # из описания извлекаем параметры, из них --- сигму
```


Красивые графики, как мне кажется, можно получить только для случая двух объясняющих переменных

Пример из документации:
```{r}
x <- rbind(matrix(rnorm(120),ncol=2),matrix(rnorm(120,mean=3),ncol=2))
y <- matrix(c(rep(1,60),rep(-1,60)))
 
svp <- ksvm(x,y,type="C-svc")
plot(svp,data=x)
```

Закрашенные треугольники и кружочки --- это опорные вектора, т.е. те точки, которые оказались на границе разделяющей полосы в спрямляющем пространстве.


### Выбор между точностью подгонки и простотой модели

Во многих моделях есть параметр, отвечающий за простоту модели.  Чем проще модель, тем хуже модель описывает выборку, по которой она оценивалась.

В линейной регрессии параметр сложности модели --- это количество регрессоров, $k$. Чем больше регрессоров, тем ниже сумма квадратов остатков, $RSS$.

На гистограмме параметр сложности --- число столбцов.

В ridge regression и LASSO параметр простоты --- это $\lambda$. Действительно, LASSO минимизирует
\[
\sum_{i=1}^n (y_i-\hat{y}_i)^2+\lambda \sum_{j=1}^k |\hat{\beta}_j|
\]
Значит, чем больше параметр $\lambda$, тем больше стремление алгоритма LASSO занулить некоторые $\hat{\beta}_j$.

В SVM за "простоту" отвечают $\sigma$ и $C$.

Этот параметр сложности не так легко оценить, как коэффициенты модели. Если наивно попытаться выбрать параметр сложности так, чтобы модель как можно лучше описывала бы выборку, по которой она оценивалась... То ничего хорошего не выйдет. Окажется, что оптимальное количество регрессоров равно плюс бесконечности, а оптимальное $\lambda$ в LASSO равно нулю. В регрессии одно из решений этой проблемы --- это проверка гипотез о значимости коэффициентов.

Есть и универсальный способ выбора сложности модели --- кросс валидация (перекрёстная проверка, cross validation). Её идея состоит в том, что надо оценивать качество прогнозов не по той же выборке, на которой оценивалась модель, а на новых наблюдениях.

### k-кратная кросс-валидация

* Разбиваем случайным образом всю выборку на k частей. Сразу мораль: используйте `set.seed()` для воспроизводимости эксперимента.
* Прогнозы для первой части выборки строим, оценивая модель по наблюдениям всех остальных частей. Прогнозы для второй части выборки строим, оценивая модель по наблюдениям всех частей кроме второй. И так далее. 
* Получаем по одному прогнозу для каждого наблюдения. 
* Считаем сумму квадратов ошибок прогнозов или другой показатель их качества.

Популярные значения k:
* 10-кратная кроссвалидация
* k равно числу наблюдений. Т.е. модель оценивается по всем наблюдениям кроме одного. Для этого невключенного наблюдения считается ошибка прогноза. Исключая по очереди то одно, то другое наблюдение, получаем ошибку прогноза для каждого наблюдения. По этим ошибкам считаем сумму квадратов. При этом подходе алгоритм не является случайным и зачастую есть готовые формулы для суммы квадратов ошибок прогнозов.


### Кросс-валидация на примере SVM



Создаём табличку перебираемых $C$ и $\sigma$:
```{r}
C <- c(1,10,100)
sigma <- c(0.1,1,10)
d <- expand.grid(C,sigma) # Случайно не декартово ли произведение это? :)
colnames(d) <- c("C","sigma")
head(d)
```

Пробуем 10-кратную кросс-валидацию для конкретных $C$ и $\sigma$:
```{r}
set.seed(33222233) # любимый seed Фрекен Бок
m1 <- ksvm(data=h,brick~.,
           kernel=rbfdot,
           kpar=list(sigma=1),
           C=1,cross=10)
cross(m1) # cross validation error
```


Для удобства оформляем это в функцию двух переменных
```{r}
f_cross <- function(sig,C) {
  model <- ksvm(data=h,brick~.,
             kernel=rbfdot,
             kpar=list(sigma=sig),
             C=C,cross=10)
  return(cross(model))

}
f_cross(1,1) # тестируем сделанную функцию
```

Применяем её ко всем возможным $C$ и $\sigma$
```{r}
d <- ddply(d,~C+sigma,transform,cr=f_cross(sigma,C))
head(d)
```



### Недостатки SVM в задаче классификации

* Оптимальная разделяющая гиперплоскость не даёт простой формулы, описывающей зависимость $y$ от регрессоров. Нет оценок, которые бы легко интерпретировались.
* Прогноз по SVM выглядит как "да" или "нет", а не как вероятность. Есть способы всё-таки достать вероятности.
* Напрямую SVM применима только для двух классов. Есть способы расширить её на много классов.


### Как из SVM получить прогнозные вероятности?

Скомбинировав SVM и logit, можно в каком-то смысл оценить $P(y_i=1)$ с помощью SVM. Сначала нужно построить прогноз по SVM для всех объектов в выборке. Затем оценить logit модель для прогнозов SVM. Зависимой переменной в logit модели выступает не исходный вектор $y$, а вектор оценок $\hat{y}_{SVM}$. И с помощью этой logit-модели можно оценивать вероятности и строить доверительные интервалы. 

Этот подход уже автоматизирован:
```{r}
m1 <- ksvm(brick~.,data=train.h,kernel="rbfdot",kpar=list(sigma=0.05),C=5,prob.model=TRUE)

probs <- predict(m1, test.h,type="probabilities")
head(probs)
```

Используя этот подход можно построить и кривые предельных эффектов.


### Как применить SVM, если $y$ принимает больше двух значений?

Если $y$ принимает больше двух значений, то можно использовать метод "все против всех":
* Для каждой пары возможных значений $y$, $y=a$ и $y=b$, составить выборку, содержащую только эти значения $y$
* На каждой из этих выборок запустить SVM, которая будет выбирать из двух значений $y$ одно
* В результате мы получили кворум SVM-ок, голосующих каждая за свой прогноз $y$. Выбираем прогноз простым большинством голосов.

Естественно, это уже автоматизировано. Есть и другие методы.

### SVM на практике

Чайники используют следующую процедуру оценивания SVM:

* Оценить несколько SVM с параметрами "от фонаря"
* Выбрать наилучшую

Суровые челябинские жители советают делать так:

* Нормировать все переменные. Например, можно вычитать среднее и делить на корень из дисперсии. Можно все переменные привести к диапазону $[0;1]$. Гуру постигшие абсолют рекомендуют диапазон $[0;10]$, как более понятный простым смертным.
* Поделить выборку на две части, обучающую и тестовую
* По обучающей части оценить SVM с гауссовским ядром, `rbfdot` в пакете `kernlab`
* Параметры $C$ и $\sigma$ подобрать с помощью кросс-валидации
* Оценить прогнозную силу полученной модели по тестовой выборке
* Если набор данных слишком большой и кросс-валидация занимает _не меряно_ времени, то её можно делать на небольшой случайной подвыборке обучающей части.

### Ссылки:

[A Practical Guide to Support Vector Classiffication](http://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf), отсюда взята инструкция по практике SVM 

[A User's Guide to Support Vector Machines](http://pyml.sourceforge.net/doc/howto.pdf) --- картинки, помогающие понять разницу гауссовского ядра для разных $C$ и $\sigma$


