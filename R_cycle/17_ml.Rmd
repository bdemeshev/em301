# Семинар по методу максимального правдоподобия

```{r}
library(maxLik)
```

## Реализуем ML своими руками через функцию 'optim'
Наши данные подчиняются следующему закону:
$x_i=i$, $y_i=4.2 x_i+\varepsilon_i$, $\varepsilon_i \sim N(0,9)$, $i=1..100$.

Создаём искуственно такие данные:
```{r "creation of the Earth in 4 lines of code"}
n.obs <- 100

x <- 1:n.obs
eps <- rnorm(n.obs,mean=0,sd=3)
y <- 4.2*x+eps
```

А теперь сделаем вид, что мы забыли, чему равны истинные значения $\beta$ и $\sigma^2$ и оценим модель 
$y_i=\beta x_i +\varepsilon_i$, где $\varepsilon_i\sim N(0,\sigma^2)$:


Заметим, что параметр $\beta$ произвольный, а параметр $\sigma^2>0$. Чтобы автоматически получать только положительные оценки параметра, часто используют простой трюк --- перепараметризацию. То есть вводят новый параметр, равный логарифму неотрицательного параметра, $a=\log(\sigma^2)$. Тогда $\sigma^2 = \exp(a)$ будет автоматом положительным. Этот трюк позволяет избежать лишних ошибок при максимизации фукнции: мы гарантируем то, что алгоритм будет перебирать только положительные $\sigma^2$. 

```{r "minus likelihood function"}
m_log_lik <- function(params, y.in, x.in) {
  # это функция правдоподобия, она зависит от параметров и данных
  beta <- params[1]
  s2 <- exp(params[2]) 
  # этот трюк полезен, чтобы параметр s2 был заведомо неотрицательным
  
  res <- -0.5*log(s2)*length(y.in)-0.5/s2*sum((y.in-beta*x.in)^2)
  # R умеет только минимизировать функции, поэтому возьмем функцию с минусом
  return(-res)
}
```

```{r "optimization"}
opt.res <- optim(fn=m_log_lik,par=c(3,0),y.in=y,x.in=x,hessian=T)
# с(3,0) - это стартовая точка. Лучшее её выбирать поближе к глобальному экстремуму. Если даже примерно не ясно, где находится глобальный экстремум, то попробуйте несколько разных стартовых значений, чтобы не попасться в локальный. Мы выбрали от фонаря.
```

```{r "extract results"}
opt.res$hessian # гессиан в точке минимума
opt.res$par # точка минимума
opt.res$value # минимум функции
```

```{r "compute confidence interval"}
var.hat <- solve(opt.res$hessian) # оценка ковариационной матрицы


estimates <- opt.res$par # вектор оценок неизвестных параметров
se <- sqrt(diag(var.hat)) # вектор ст. ошибок, корни из диагонали ковариационной матрицы


ci.left <- estimates - 1.96 * se
ci.right <- estimates + 1.96 * se

coef.table <- data.frame(estimates,se,ci.left,ci.right
                         )
rownames(coef.table) <- c("beta","log(sigma^2)")
colnames(coef.table) <- c("Оценка","Ст. ошибка","Левая граница","Правая граница")

coef.table
```

Можно найти кучу примеров ["MLE in R" в гугле](https://www.google.ru/search?q=mle+in+R)


## Реализуем ML с помощью пакета `maxLik`

```{r "likelihood function"}
log_lik <- function(params, y.in, x.in) {
  # это функция правдоподобия, она зависит от параметров и данных
  beta <- params[1]
  s2 <- exp(params[2]) 
  # этот трюк полезен, чтобы параметр s2 был заведомо неотрицательным
  
  res <- -0.5*log(s2)*length(y.in)-0.5/s2*sum((y.in-beta*x.in)^2)
  return(res)
}
```

Оцениваем модель с помощью ML:
```{r}
ml.results <- maxLik(log_lik,start=c(0,0),y.in=y,x.in=x)
```

Выводим привычную табличку-отчёт:
```{r}
summary(ml.results)
```


## Челябинские функции настолько суровы...

К сожалению, алгоритмы поиска экстремума еще далеки от совершенства. Есть функции, у которых человек легко найдёт экстремум устно, а популярные алгоритмы не смогут найти экстремум.

Например, $f(x_1,x_2,x_3)=0.01\cdot (x_1-0.5)^2+ |x_1^2-x_2|+|x_1^2-x_3|$

Если присмотреться, то легко заметить, что точка минимума $x_1=0.5$, $x_2=0.25$, $x_3=0.25$.

А что скажет компьютер?

```{r}
bad.f <- function (x) {
  return(0.01*(x[1]-0.5)^2+abs(x[1]^2-x[2])+abs(x[1]^2-x[3]))
}
res <- optim(fn=bad.f,par=c(0,0,0))
res
```

Можно попробовать другие алгоритмы, встроенные в функцию `optim`.


Замечания:
* Лучше добавлять в функции комментарии и проверку на корректность вводимых значений.










